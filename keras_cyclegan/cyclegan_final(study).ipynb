{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Convolutional Neural Network (CNN)을 구성하는 레이어들 불러오기\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, Dropout, Flatten, Dense\n",
    "\n",
    "'''\n",
    "Conv2D: 2D 컨볼루션 레이어입니다. 이미지 등의 2차원 데이터에서 특징을 추출하는데 사용됩니다.\n",
    "\n",
    "Conv2DTranspose: 전치 합성곱 레이어로서, 역으로 컨볼루션 연산을 수행합니다. 이를 통해 입력을 더 큰 공간으로 확장할 수 있습니다.\n",
    "\n",
    "UpSampling2D: 2D 업샘플링 레이어입니다. 입력 데이터의 크기를 확장하여 공간 해상도를 높일 수 있습니다.\n",
    "\n",
    "Dropout: 신경망에서 과적합을 방지하기 위해 사용되는 레이어입니다. 특정 확률로 뉴런을 무작위로 비활성화합니다.\n",
    "\n",
    "Flatten: 다차원 배열을 1차원 배열로 변환하는 레이어입니다. 주로 완전 연결 레이어에 입력으로 사용됩니다.\n",
    "\n",
    "Dense: 완전 연결 레이어입니다. 모든 입력 뉴런이 출력 뉴런에 연결되어 있는 레이어입니다.\n",
    "\n",
    "다운샘플링(Down-sampling) : Image 크기를 줄여가며 특징을 추출하는 과정이다.\n",
    "\n",
    "업샘플링(Up-sampling) : 원래 Image 크기로 복원하는 과정이다.\n",
    "\n",
    "'''\n",
    "# Instance Normalization 레이어 불러오기 (일반적인 Batch Normalization과 다른 정규화 방법)\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "\n",
    "# JSON 형식으로부터 모델을 로드하기 위한 함수 불러오기\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# 진행 상황을 시각적으로 표시하기 위한 tqdm 라이브러리 불러오기\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 파일 경로를 찾기 위한 glob 라이브러리 불러오기\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make CycleGAN Model\n",
    "'''\n",
    "def cyclegan():\n",
    "    # 입력 이미지의 형태\n",
    "    img_shape = (128, 128, 3)\n",
    "        \n",
    "    ### 1. Discriminator 구성 (이미지가 실제인지 가짜인지 판별 역할)\n",
    "    D_A = discriminator()  # discriminator A\n",
    "    D_B = discriminator()  # discriminator B\n",
    "    \n",
    "    # Discriminator만 업데이트, Generator는 업데이트 X\n",
    "    D_A.trainable = True\n",
    "    D_B.trainable = True \n",
    "    \n",
    "    D_A.summary()  # discriminator A의 구조 출력\n",
    "    \n",
    "    # Loss와 Optimizer 설정 학습률,베타 - 그래디언트 영향정도\n",
    "    D_A.compile(loss='mse', optimizer=Adam(0.0002, 0.5))\n",
    "    D_B.compile(loss='mse', optimizer=Adam(0.0002, 0.5))\n",
    "  \n",
    "    ### 2. Generator 파이프라인 구성\n",
    "    G_A = generator()  # 스타일 A에서 B로 변환하는 generator\n",
    "    G_B = generator()  # 스타일 B에서 A로 변환하는 generator\n",
    "    \n",
    "    # Generator 를 학습할 때는 discriminator  freeze\n",
    "    D_A.trainable = False  \n",
    "    D_B.trainable = False \n",
    "    \n",
    "    # 입력 이미지 크기 정의\n",
    "    real_A = Input(shape=img_shape)\n",
    "    real_B = Input(shape=img_shape)\n",
    "    \n",
    "    # A에서 B로의 변환 (A사진에 B스타일 적용)\n",
    "    fake_B = G_A(real_A)     # 스타일 A -> B 변환\n",
    "    f_label_B = D_B(fake_B)  # discriminator B의 fake 이미지 분류 예측값\n",
    "    cycle_A = G_B(fake_B)    # fake 이미지를 원래 형태로 복원시킨 이미지\n",
    "    id_A = G_B(real_A)       # identity mapping : 입력 이미지를 그대로 출력하여 모델 안정성(이해x)\n",
    "    \n",
    "    # B에서 A로의 변환\n",
    "    fake_A = G_B(real_B)\n",
    "    f_label_A = D_A(fake_A)\n",
    "    cycle_B = G_A(fake_A)\n",
    "    id_B = G_A(real_B)\n",
    "    \n",
    "    # Generator 파이프라인 모델 정의\n",
    "    gen_pipe = Model(inputs=[real_A, real_B], outputs=[f_label_A, f_label_B, cycle_A, cycle_B, id_A, id_B])\n",
    "    # 손실함수 값이 작으면 목표 이미지 간의 차이를 작게 만들지만 문제 발생 \n",
    "    # 1.과적합   2. 학습 불안정성   3. 품질 감소\n",
    "    gen_pipe.compile(loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], \n",
    "                     loss_weights=[1, 1, 10, 10, 1, 1], \n",
    "                     optimizer=Adam(0.0002, 0.5))\n",
    "    gen_pipe.summary()  # Generator 파이프라인의 구조 출력\n",
    "    \n",
    "    return D_A, D_B, G_A, G_B, gen_pipe\n",
    "\n",
    "\n",
    "'''\n",
    "Discriminator Structure\n",
    "'''\n",
    "def discriminator():\n",
    "    # 입력 이미지를 받는 레이어 정의\n",
    "    inp = Input(shape=(128, 128, 3))\n",
    "\n",
    "    # Convolutional 레이어를 통해 이미지 처리\n",
    "    conv1 = Conv2D(32, kernel_size=4, strides=2, padding='same')(inp)  # w/o instancenorm\n",
    "    conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(64, kernel_size=4, strides=2, padding='same')(conv1)\n",
    "    conv2 = LeakyReLU(alpha=0.2)(InstanceNormalization()(conv2))  # Instance Normalization 적용\n",
    "    \n",
    "    conv3 = Conv2D(128, kernel_size=4, strides=2, padding='same')(conv2)\n",
    "    conv3 = LeakyReLU(alpha=0.2)(InstanceNormalization()(conv3))\n",
    "\n",
    "    # Discriminator의 출력 레이어 정의\n",
    "    outp = Conv2D(1, kernel_size=4, padding='same')(conv3)  # label 추출 -> pieces로 분석\n",
    "\n",
    "    # Model 객체 생성\n",
    "    d = Model(inp, outp)\n",
    "    return d\n",
    "\n",
    "\n",
    "'''\n",
    "Generator Structure\n",
    "'''\n",
    "# 생성자 함수 : 이미지의 크기를 키워가면서 디테일을 추가하여 이미지를 생성\n",
    "# Instance Normalization : 변환된 이미지의 통계적인 특성을 안정화 (Image Style Transfer 성능 올리기 위함)\n",
    "def generator():    \n",
    "    inp = Input(shape=(128, 128, 3))\n",
    "\n",
    "    # Downsample 함수 정의 (Discriminator구조와 일치)\n",
    "    def downsample(layer, filters, kernels):\n",
    "        x = Conv2D(filters, kernel_size=kernels, strides=2, padding='same')(layer)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        return x\n",
    "    \n",
    "    # Upsample 함수 정의\n",
    "    # Usampling 단계이기 때문에 strides 는 필요X > Usampling2D(size = ) : 이미지 확대 작업\n",
    "    def upsample(layer, connect, filters, kernels): \n",
    "        x = Conv2D(filters, kernel_size=kernels, padding='same')(UpSampling2D(size=2)(layer))\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        return concatenate([x, connect])\n",
    "\n",
    "    # 다양한 레이어를 통해 이미지 처리\n",
    "    conv1 = downsample(inp, 32, 4)\n",
    "    conv2 = downsample(conv1, 64, 4)\n",
    "    conv3 = downsample(conv2, 128, 4)\n",
    "    conv4 = downsample(conv3, 32*8, 4)\n",
    "    conv5 = upsample(conv4, conv3, 128, 4)\n",
    "    conv6 = upsample(conv5, conv2, 64, 4)\n",
    "    conv7 = upsample(conv6, conv1, 32, 4)\n",
    "    \n",
    "    # Generator의 출력 레이어 정의\n",
    "    outp = Conv2D(3, kernel_size=4, padding='same', activation='tanh')(UpSampling2D(size=2)(conv7))  # RGB 채널로 복원\n",
    "\n",
    "    # Model 객체 생성\n",
    "    g = Model(inp, outp)\n",
    "    return g\n",
    "\n",
    "\n",
    "'''\n",
    "Train Model\n",
    "'''\n",
    "# 스타일 변환의 경우, 개별 이미지에 대한 변환을 수행하므로 batch_size = 1 로 설정. 미니배치X\n",
    "def main(styles, photos, epoch, batch_size=1): \n",
    "    \"\"\"\n",
    "    CycleGAN을 활용한 스타일 변환을 수행하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "        styles (numpy.ndarray): 스타일 이미지 데이터셋.\n",
    "        photos (numpy.ndarray): 훈련용 사진 이미지 데이터셋.\n",
    "        epoch (int): 전체 훈련 반복 횟수.\n",
    "        batch_size (int, optional): 한 번에 처리할 이미지의 개수. 기본값은 1.\n",
    "\n",
    "    Returns:\n",
    "        G_B (keras.Model): 스타일 변환된 이미지를 생성하는 Generator 모델.\n",
    "        losses_D_A (list): Discriminator A의 손실 값 리스트.\n",
    "        losses_D_B (list): Discriminator B의 손실 값 리스트.\n",
    "        losses_G (list): Generator의 손실 값 리스트.\n",
    "    \"\"\"\n",
    "\n",
    "    # 전체 데이터셋 중, 최소 크기를 기준으로 batch_size로 나눈 값 계산 (ex.사진 10개/sytle 100개 >10번 학습)\n",
    "    end = int(min(styles.shape[0], photos.shape[0]) / batch_size)\n",
    "\n",
    "    # Discriminator와 Generator 모델 초기화\n",
    "    D_A, D_B, G_A, G_B, gen_pipe = cyclegan()\n",
    "\n",
    "    # Discriminator의 실제와 가짜 이미지 레이블 초기화\n",
    "    real_label = np.ones((batch_size, 16, 16, 1))\n",
    "    fake_label = np.zeros((batch_size, 16, 16, 1))\n",
    "\n",
    "    # 손실 값을 저장할 리스트 초기화\n",
    "    losses_D_A = []\n",
    "    losses_D_B = []\n",
    "    losses_G = []\n",
    "\n",
    "    for j in range(epoch):\n",
    "        for i in range(end):\n",
    "            # 무작위로 스타일 이미지와 훈련용 사진 이미지 선택\n",
    "            style = styles[np.random.randint(0, end, size=batch_size)]\n",
    "            photo = photos[np.random.randint(0, end, size=batch_size)]\n",
    "            \n",
    "            # Generator에서 fake_A, fake_B 생성\n",
    "            stop_fake = G_A.predict(style)\n",
    "            ptos_fake = G_B.predict(photo)\n",
    "\n",
    "            # Discriminator 학습: fake는 0, real은 1값이 나오도록\n",
    "            # train_on_batch : 단일 배치에 대해 모델을 학습 (데이터 하나씩 모델 가중치 업데이트)\n",
    "            \n",
    "            # 실제 데이터에 대한 훈련 > 예측 최적화\n",
    "            loss_D_A_r = D_A.train_on_batch(style, real_label)\n",
    "            # 가짜 데이터에 대한 훈련 > 예측 최적화\n",
    "            loss_D_A_f = D_A.train_on_batch(ptos_fake, fake_label)\n",
    "            # 손실 계산\n",
    "            loss_D_A = (loss_D_A_r + loss_D_A_f) / 2\n",
    "\n",
    "            loss_D_B_r = D_B.train_on_batch(photo, real_label)\n",
    "            loss_D_B_f = D_B.train_on_batch(stop_fake, fake_label)\n",
    "            loss_D_B = (loss_D_B_r + loss_D_B_f) / 2\n",
    "            \n",
    "            # 손실 값을 리스트에 추가\n",
    "            losses_D_A.append(loss_D_A)\n",
    "            losses_D_B.append(loss_D_B)\n",
    "            \n",
    "            # Generator 학습: pipeline에 들어갔을 때 discriminator들이 fake에 대해 1을 출력하도록\n",
    "            loss_G = gen_pipe.train_on_batch([style, photo], [real_label, real_label, style, photo, style, photo])\n",
    "            losses_G.append(loss_G)\n",
    "            \n",
    "            # 이미지 출력 및 저장  # 역-scale [-1,1] > [0,255]\n",
    "            if i % 150 == 0:\n",
    "                print(\"epoch : {}, iteration: {}, D Loss : {:.3f}, G Loss(photo->style) : {:.3f}\".format(j, i, (loss_D_A + loss_D_B) / 2, loss_G[0]))\n",
    "                fig = plt.figure()\n",
    "                ax = plt.subplot(1, 3, 1)\n",
    "                ax.set_title(\"Original\")\n",
    "                ax.imshow(((photo.reshape(128, 128, 3) + 1) * 127.5).astype(np.uint8))  # scale back\n",
    "\n",
    "                ax = plt.subplot(1, 3, 2)\n",
    "                ax.set_title(\"Transferred\")\n",
    "                ax.imshow(((ptos_fake.reshape(128, 128, 3) + 1) * 127.5).astype(np.uint8))\n",
    "                \n",
    "                ax = plt.subplot(1, 3, 3)\n",
    "                ax.set_title(\"Cycle\")\n",
    "                ax.imshow(((G_A.predict(ptos_fake).reshape(128, 128, 3) + 1) * 127.5).astype(np.uint8))\n",
    "                \n",
    "                fig.savefig(\"trainprocess/epoch{}batch{}\".format(j, i))\n",
    "                plt.close()\n",
    "                \n",
    "    return G_B, losses_D_A, losses_D_B, losses_G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getimage( t, batch=1): #dataset_name,\n",
    "    A = glob('./datasets/trainA/*') # 해당 경로파일 목록 반환\n",
    "    B = glob('./datasets/trainB/*')\n",
    "    \n",
    "    if t=='style':\n",
    "        where = A\n",
    "    else:\n",
    "        where = B\n",
    "        \n",
    "    images = []\n",
    "    for _ in tqdm(where):\n",
    "        image = cv2.imread(_)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #bgr -> rgb\n",
    "        image = cv2.resize(image, dsize=(128, 128))\n",
    "        images.append(image)\n",
    "     #scale to -1~1 : 훈련과정에서 안정성과 효율성, 역전파 최적화\n",
    "    return np.array(images)/127.5-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▌                                                | 249/630 [00:39<00:56,  6.73it/s]"
     ]
    }
   ],
   "source": [
    "styles = getimage(batch = 1, t = 'style')\n",
    "photos = getimage(batch = 1, t = 'photo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    transfer_generator, losses_D_A, losses_D_B, losses_G = main(styles, photos, epoch=200, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_loss=[i[0] for i in np.array(losses_G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.plot(np.add(losses_D_A,losses_D_B)/2,label=\"Discriminator\")\n",
    "plt.plot(G_loss,label=\"Generator\")\n",
    "plt.legend()\n",
    "plt.title('Discriminator loss and Generator loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_json = transfer_generator.to_json()\n",
    "with open(\"generator_git.json\",\"w\") as file:\n",
    "    file.write(G_json)\n",
    "transfer_generator.save_weights(\"generator_git.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('generator_git.json','r')\n",
    "g_model = file.read()\n",
    "file.close()\n",
    "generator = model_from_json(g_model,custom_objects={'InstanceNormalization':InstanceNormalization\n",
    "})\n",
    "generator.load_weights(\"generator_git.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check test data\n",
    "test = glob('./datasets/testB/*')\n",
    "for _ in test[0:5]:\n",
    "    image = cv2.imread(_)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #bgr->rgb\n",
    "    image = cv2.resize(image, dsize=(128,128))\n",
    "    image = image/127.5-1\n",
    "    image = image.reshape(1,128,128,3)\n",
    "    styletransferred = generator.predict(image)\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(((image.reshape(128,128,3)+1)*127.5).astype(np.uint8))\n",
    "    ax[1].imshow(((styletransferred.reshape(128,128,3)+1)*127.5).astype(np.uint8))\n",
    "    plt.title('transferred')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
